<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Logs</title>
</head>
<body>
    <h2>1.1 </h2>
    <xmp>
        import cv2
        import numpy as np
        image1_path = r'C:\Users\vicky\OneDrive\Desktop\clg\Labs\7th sem\IPCV\horse.jpg'
        image2_path = r'C:\Users\vicky\OneDrive\Desktop\clg\Labs\7th sem\IPCV\image.jpg'
        M1 = cv2.imread(image1_path)
        M2 = cv2.imread(image2_path)
        M2 = cv2.resize(M2, (M1.shape[1], M1.shape[0]))
        Out = cv2.absdiff(M1, M2)
        cv2.imshow('Absolute Difference', Out)
        cv2.waitKey(0)
        cv2.destroyAllWindows()
    </xmp>
    <h2>1.2 feature extraction HOG</h2>
    <xmp>
        import cv2
        import numpy as np
        import matplotlib.pyplot as plt
        # from skimage.feature import hog
        from skimage import exposure
        import mahotas
        image_path = r'C:\Users\vicky\OneDrive\Desktop\clg\Labs\7th sem\IPCV\image.jpg'
        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
        image = cv2.resize(image, (64, 128))
        gradient_x = cv2.Sobel(image, cv2.CV_64F, 1, 0,ksize=3)
        gradient_y = cv2.Sobel(image, cv2.CV_64F, 0, 1,ksize=3)
        print("The gradients are : ", gradient_x, gradient_y)
        magnitude = np.sqrt(gradient_x**2 + gradient_y**2)
        orientation = np.arctan2(gradient_y, gradient_x) * (180 / np.pi) 
        print("Magintude: ", magnitude)
        print("Gradient: ",orientation)
        
        from skimage.feature import hog
        features, hog_image = hog(image,visualize=True)
        
        hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0,10))
        
        plt.figure()
        plt.imshow(image, cmap=plt.cm.gray)
        plt.title('Original Image')
        plt.axis('off')
        plt.show()
        plt.figure()
        plt.imshow(hog_image_rescaled, cmap=plt.cm.gray)
        plt.title('HOG Image')
        plt.axis('off')
        plt.show()
        
        
        print(features)
        # Print the HOG feature vector length
        print("HOG feature vector length:", len(features))        
    </xmp>
    <h2>2.1 contrast stretching linear filtering</h2>
    <xmp>
        import cv2
        import numpy as np
        import matplotlib.pyplot as plt
        
        # Load the image in grayscale
        image_path = r'image.jpg'  # Replace with your image path
        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
        
        # 1) Contrast Stretching
        # Find the minimum and maximum pixel values
        min_val = np.min(image)
        max_val = np.max(image)
        
        # Apply contrast stretching
        stretched_image = ((image - min_val) / (max_val - min_val) * 255).astype(np.uint8)
        
        # 2) Linear Filtering (Gaussian filter)
        filtered_image = cv2.GaussianBlur(image, (5, 5), 0)
        
        # Show the original image and its histogram
        plt.figure(figsize=(6, 6))
        plt.imshow(image, cmap='gray')
        plt.title('Original Image')
        plt.axis('off')
        plt.show()
        
        plt.figure(figsize=(6, 6))
        plt.hist(image.ravel(), bins=256, range=(0, 255))
        plt.title('Histogram of Original Image')
        plt.show()
        
        # Show the contrast stretched image and its histogram
        plt.figure(figsize=(6, 6))
        plt.imshow(stretched_image, cmap='gray')
        plt.title('Contrast Stretched Image')
        plt.axis('off')
        plt.show()
        
        plt.figure(figsize=(6, 6))
        plt.hist(stretched_image.ravel(), bins=256, range=(0, 255))
        plt.title('Histogram of Contrast Stretched Image')
        plt.show()
        
        # Show the filtered image and its histogram
        plt.figure(figsize=(6, 6))
        plt.imshow(filtered_image, cmap='gray')
        plt.title('Filtered Image (Gaussian Blur)')
        plt.axis('off')
        plt.show()
        
        plt.figure(figsize=(6, 6))
        plt.hist(filtered_image.ravel(), bins=256, range=(0, 255))
        plt.title('Histogram of Filtered Image')
        plt.show()
    </xmp>
<h2>2.2 horse and human clasify</h2>
<xmp>
    import tensorflow as tf
    from tensorflow.keras import layers, models
    from tensorflow.keras.applications import ResNet50
    from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
    from tensorflow.keras.optimizers import Adam
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    from sklearn.metrics import confusion_matrix, accuracy_score
    
    # Load the dataset
    dataset_dir = r"C:\Users\vicky\Downloads\Mid-model Exam\Mid-model Exam\horse-or-human\horse-or-human"  # Replace with actual path
    train_data = tf.keras.utils.image_dataset_from_directory(
        dataset_dir,
        image_size=(128, 128),
        batch_size=32,
        label_mode='binary',
        validation_split=0.3,
        subset='training',
        seed=42
    )
    
    val_data = tf.keras.utils.image_dataset_from_directory(
        dataset_dir,
        image_size=(128, 128),
        batch_size=32,
        label_mode='binary',
        validation_split=0.3,
        subset='validation',
        seed=42
    )
    for images, labels in train_data.take(1):  # Take one batch from the test set
        plt.figure(figsize=(10, 10))
        for i in range(9):  
            ax = plt.subplot(3, 3, i + 1)
            plt.imshow(images[i].numpy().astype("uint8"))
            plt.title(labels[i])
            plt.axis("off")
        plt.show()
    # Normalize data
    normalization_layer = layers.Rescaling(1.0 / 255)
    train_data = train_data.map(lambda x, y: (normalization_layer(x), y))
    val_data = val_data.map(lambda x, y: (normalization_layer(x), y))
    
    # Modify ResNet50 for binary classification
    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(128, 128, 3))
    base_model.trainable = False  # Freeze the base model
    
    # Add custom layers on top
    model = models.Sequential([
        base_model,
        GlobalAveragePooling2D(),
        Dense(1, activation='sigmoid')  # Output layer for binary classification
    ])
    
    # Compile the model
    model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])
    
    # Train the model
    history = model.fit(train_data, validation_data=val_data, epochs=3)
    
    # Evaluate the model on the training data to get training accuracy
    train_loss, train_acc = model.evaluate(train_data)
    print(f"\nTraining Accuracy: {train_acc:.2f}")
    
    # Evaluate the model on the validation data to get testing accuracy
    val_loss, val_acc = model.evaluate(val_data)
    print(f"\nValidation (Testing) Accuracy: {val_acc:.2f}")
    
    # Predict labels on the validation dataset
    y_pred = np.round(model.predict(val_data).flatten())
    y_true = np.concatenate([label.numpy() for _, label in val_data], axis=0)
    
    # Calculate and plot confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['Horse', 'Human'], yticklabels=['Horse', 'Human'])
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title("Confusion Matrix")
    plt.show()
    
</xmp>
<h2>3.1 geometrical transformation</h2>
<xmp>
    import cv2
    import numpy as np
    import matplotlib.pyplot as plt
    
    # Load the image
    image_path = r'image.jpg'  # Replace with the path to your image
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB for displaying with matplotlib
    
    # Display original image
    plt.figure(figsize=(10, 10))
    plt.subplot(2, 2, 1)
    plt.imshow(image)
    plt.title("Original Image")
    plt.axis('off')
    
    # 1. Scaling
    scale_percent = 50  # Scale image to 50% of original size
    width = int(image.shape[1] * scale_percent / 100)
    height = int(image.shape[0] * scale_percent / 100)
    scaled_image = cv2.resize(image, (width, height), interpolation=cv2.INTER_AREA)
    
    plt.subplot(2, 2, 2)
    plt.imshow(scaled_image)
    plt.title("Scaled Image (50%)")
    plt.axis('off')
    
    # 2. Rotation
    angle = 45  # Rotate the image by 45 degrees
    (h, w) = image.shape[:2]
    center = (w // 2, h // 2)
    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)
    rotated_image = cv2.warpAffine(image, rotation_matrix, (w, h))
    
    plt.subplot(2, 2, 3)
    plt.imshow(rotated_image)
    plt.title("Rotated Image (45 degrees)")
    plt.axis('off')
    
    # 3. Shearing
    shear_factor = 0.2
    M_shear = np.array([[1, shear_factor, 0],
                        [shear_factor, 1, 0],
                        [0, 0, 1]], dtype=float)
    
    sheared_image = cv2.warpPerspective(image, M_shear, (int(w * (1 + shear_factor)), int(h * (1 + shear_factor))))
    
    plt.subplot(2, 2, 4)
    plt.imshow(sheared_image)
    plt.title("Sheared Image")
    plt.axis('off')
    
    # Display all images
    plt.tight_layout()
    plt.show()
    
</xmp>
<h2>3.2 feature extraction SIFT</h2>
<xmp>
    import cv2
    import matplotlib.pyplot as plt
    
    # Load the image
    image_path = 'image.jpg'  # Replace with the path to your image
    image = cv2.imread(image_path)
    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    
    # Step 1: Constructing a Scale Space and Detecting Keypoints
    sift = cv2.SIFT_create()
    keypoints = sift.detect(gray_image, None)
    
    # Step 2: Keypoint Localization (part of SIFT's internal process)
    # Keypoints are already localized during the `detect` method
    
    # Step 3 & 4: Orientation Assignment and Keypoint Descriptor Computation
    keypoints, descriptors = sift.compute(gray_image, keypoints)
    
    # Draw the keypoints on the original image
    image_with_keypoints = cv2.drawKeypoints(image, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)
    
    # Display the image with keypoints
    plt.figure(figsize=(10, 10))
    plt.imshow(cv2.cvtColor(image_with_keypoints, cv2.COLOR_BGR2RGB))
    plt.title("Image with SIFT Keypoints")
    plt.axis('off')
    plt.show()
    
    # Print out some information about keypoints and descriptors
    print(f"Number of keypoints detected: {len(keypoints)}")
    print("Descriptor shape:", descriptors.shape)
    
</xmp>
<h2>4.1 extract image from video</h2>
<xmp>
    import cv2
    import os
    
    # Video path
    video_path = 'path/to/your/video.mp4'  # Replace with the actual video path
    output_folder = 'extracted_images'  # Folder to store extracted images
    
    # Create a folder to store images if it doesn't exist
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    
    # Open the video file
    cap = cv2.VideoCapture(video_path)
    
    # Check if the video was opened successfully
    if not cap.isOpened():
        print("Error: Could not open video.")
        exit()
    
    # Get the total number of frames in the video
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f'Total frames in the video: {total_frames}')
    
    frame_count = 0
    while True:
        ret, frame = cap.read()
        if not ret:
            break  # Exit if we reach the end of the video
    
        # Save every frame as an image
        frame_filename = os.path.join(output_folder, f"frame_{frame_count:04d}.jpg")
        cv2.imwrite(frame_filename, frame)
        
        print(f"Extracting frame {frame_count + 1}/{total_frames}")
        frame_count += 1
    
    # Release the video capture object
    cap.release()
    
    print("Image extraction completed!")    
</xmp>
<h2>4.2 horse and human cnn resnet</h2>
<xmp>
    import tensorflow as tf
    from tensorflow.keras import layers, models
    from tensorflow.keras.applications import ResNet50
    from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
    from tensorflow.keras.optimizers import Adam
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    from sklearn.metrics import confusion_matrix, accuracy_score
    
    # Load the dataset
    dataset_dir = r"C:\Users\vicky\Downloads\Mid-model Exam\Mid-model Exam\horse-or-human\horse-or-human"  # Replace with actual path
    train_data = tf.keras.utils.image_dataset_from_directory(
        dataset_dir,
        image_size=(128, 128),
        batch_size=32,
        label_mode='binary',
        validation_split=0.3,
        subset='training',
        seed=42
    )
    
    val_data = tf.keras.utils.image_dataset_from_directory(
        dataset_dir,
        image_size=(128, 128),
        batch_size=32,
        label_mode='binary',
        validation_split=0.3,
        subset='validation',
        seed=42
    )
    
    # Normalize data
    normalization_layer = layers.Rescaling(1.0 / 255)
    train_data = train_data.map(lambda x, y: (normalization_layer(x), y))
    val_data = val_data.map(lambda x, y: (normalization_layer(x), y))
    
    # Modify ResNet50 for binary classification
    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(128, 128, 3))
    base_model.trainable = False  # Freeze the base model
    
    # Add custom layers on top
    model = models.Sequential([
        base_model,
        GlobalAveragePooling2D(),
        Dense(1, activation='sigmoid')  # Output layer for binary classification
    ])
    
    # Compile the model
    model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])
    
    # Train the model
    history = model.fit(train_data, validation_data=val_data, epochs=3)
    
    # Evaluate the model on the training data to get training accuracy
    train_loss, train_acc = model.evaluate(train_data)
    print(f"\nTraining Accuracy: {train_acc:.2f}")
    
    # Evaluate the model on the validation data to get testing accuracy
    val_loss, val_acc = model.evaluate(val_data)
    print(f"\nValidation (Testing) Accuracy: {val_acc:.2f}")
    
    # Predict labels on the validation dataset
    y_pred = np.round(model.predict(val_data).flatten())
    y_true = np.concatenate([label.numpy() for _, label in val_data], axis=0)
    
    # Calculate and plot confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['Horse', 'Human'], yticklabels=['Horse', 'Human'])
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title("Confusion Matrix")
    plt.show()
    
</xmp>
<h2>5 MS-COCO dataset</h2>
<xmp>
    import tensorflow as tf
    import tensorflow_datasets as tfds
    import matplotlib.pyplot as plt
    from tensorflow.keras import layers
    
    # Load the MS-COCO dataset
    dataset, info = tfds.load('coco/2017', with_info=True, as_supervised=True)
    
    # Split the dataset into training and validation (testing) sets
    train_data = dataset['train']
    val_data = dataset['validation']
    
    # Check the number of training and testing images
    train_size = info.splits['train'].num_examples
    val_size = info.splits['validation'].num_examples
    
    print(f'Training Images: {train_size}')
    print(f'Validation Images: {val_size}')
    
    def plot_images(dataset, num_images=5):
        plt.figure(figsize=(10, 10))
        for i, (image, label) in enumerate(dataset.take(num_images)):
            plt.subplot(1, num_images, i + 1)
            plt.imshow(image)
            plt.axis('off')
        plt.show()
    
    plot_images(train_data)
    
    def augment_image(image):
        image = tf.image.random_flip_left_right(image)  # Random horizontal flip
        image = tf.image.random_flip_up_down(image)    # Random vertical flip
        image = tf.image.random_contrast(image, lower=0.2, upper=0.5)  # Random contrast
        image = tf.image.random_rotation(image, 0.2)   # Random rotation
        return image
    
    # Apply augmentation on the dataset
    train_data = train_data.map(lambda image, label: (augment_image(image), label))
    val_data = val_data.map(lambda image, label: (augment_image(image), label))
    
    normalization_layer = layers.Rescaling(1.0 / 255)
    train_data = train_data.map(lambda x, y: (normalization_layer(x), y))
    val_data = val_data.map(lambda x, y: (normalization_layer(x), y))
    
    # Build a CNN model
    model = tf.keras.Sequential([
        layers.InputLayer(input_shape=(128, 128, 3)),  # Adjust size to 128x128 for simplicity
        layers.Conv2D(32, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(128, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dense(1, activation='sigmoid')  # Binary classification
    ])
    
    # Compile the model
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    
    # Train the model
    history = model.fit(train_data.batch(32), validation_data=val_data.batch(32), epochs=10)
    
    
    # Plot training and validation accuracy
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()    
</xmp>
<h2>6 BCCD dataset</h2>
<xmp>
    import tensorflow as tf
    import tensorflow_datasets as tfds
    import matplotlib.pyplot as plt
    from tensorflow.keras import layers
    
    # Load the BCCD dataset from TensorFlow Datasets
    dataset, info = tfds.load('blood_cells', with_info=True, as_supervised=True)
    
    # The dataset is split into 'train' and 'test' by default
    train_data = dataset['train']
    val_data = dataset['test']
    
    # Check the number of training and testing images
    train_size = info.splits['train'].num_examples
    val_size = info.splits['test'].num_examples
    
    print(f'Training Images: {train_size}')
    print(f'Validation Images: {val_size}')
    
    # Plot a few images from the dataset
    def plot_images(dataset, num_images=5):
        plt.figure(figsize=(10, 10))
        for i, (image, label) in enumerate(dataset.take(num_images)):
            plt.subplot(1, num_images, i + 1)
            plt.imshow(image)
            plt.axis('off')
        plt.show()
    
    plot_images(train_data)
    
    # Image Augmentation function
    def augment_image(image):
        image = tf.image.random_flip_left_right(image)  # Random horizontal flip
        image = tf.image.random_flip_up_down(image)    # Random vertical flip
        image = tf.image.random_contrast(image, lower=0.2, upper=0.5)  # Random contrast
        return image
    
    # Apply augmentation on the dataset
    train_data = train_data.map(lambda image, label: (augment_image(image), label))
    val_data = val_data.map(lambda image, label: (augment_image(image), label))
    
    # Normalize the images
    normalization_layer = layers.Rescaling(1.0 / 255)
    train_data = train_data.map(lambda x, y: (normalization_layer(x), y))
    val_data = val_data.map(lambda x, y: (normalization_layer(x), y))
    
    # Build a CNN model
    model = tf.keras.Sequential([
        layers.InputLayer(input_shape=(128, 128, 3)),  # Adjust size to 128x128 for simplicity
        layers.Conv2D(32, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(128, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dense(1, activation='sigmoid')  # Binary classification, change to softmax for multi-class
    ])
    
    # Compile the model
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    
    # Train the model
    history = model.fit(train_data.batch(32), validation_data=val_data.batch(32), epochs=10)
    
    # Plot training and validation accuracy
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()    
</xmp>
<h2>7.1 edge detection gray scale</h2>
<xmp>
    import cv2
    import numpy as np
    import matplotlib.pyplot as plt
    
    def canny_edge_detection(image):
        # Step 1: Smoothing the image using Gaussian filter
        blurred_image = cv2.GaussianBlur(image, (5, 5), 1.4)
        
        # Step 2: Compute the gradient in both the x and y directions
        grad_x = cv2.Sobel(blurred_image, cv2.CV_64F, 1, 0, ksize=3)  # Sobel operator in x direction
        grad_y = cv2.Sobel(blurred_image, cv2.CV_64F, 0, 1, ksize=3)  # Sobel operator in y direction
        
        # Step 3: Calculate the magnitude and angle of the gradient
        magnitude = np.sqrt(grad_x**2 + grad_y**2)  # Gradient magnitude
        angle = np.arctan2(grad_y, grad_x)  # Gradient direction
        
        # Step 4: Non-Maximum Suppression (thinning of edges)
        non_max_suppressed = non_maximum_suppression(magnitude, angle)
        
        # Step 5: Apply Double Thresholding
        edges = double_threshold(non_max_suppressed)
        
        # Step 6: Edge Tracking by Hysteresis (optional)
        final_edges = edge_tracking_by_hysteresis(edges)
        
        return final_edges
    
    def non_maximum_suppression(magnitude, angle):
        rows, cols = magnitude.shape
        suppressed_image = np.zeros_like(magnitude)
        
        # Quantize the angle to 4 directions: 0, 45, 90, 135 degrees
        angle = np.rad2deg(angle) % 180  # Convert to degrees and ensure it's between 0-180 degrees
        angle[angle < 0] += 180
        
        for i in range(1, rows-1):
            for j in range(1, cols-1):
                # Get the gradient directions and suppress accordingly
                q = 255
                r = 255
                
                # Angle 0 degrees (horizontal edge)
                if (0 <= angle[i, j] < 22.5) or (157.5 <= angle[i, j] <= 180):
                    q = magnitude[i, j + 1]
                    r = magnitude[i, j - 1]
                # Angle 45 degrees (diagonal edge)
                elif (22.5 <= angle[i, j] < 67.5):
                    q = magnitude[i + 1, j - 1]
                    r = magnitude[i - 1, j + 1]
                # Angle 90 degrees (vertical edge)
                elif (67.5 <= angle[i, j] < 112.5):
                    q = magnitude[i + 1, j]
                    r = magnitude[i - 1, j]
                # Angle 135 degrees (diagonal edge)
                elif (112.5 <= angle[i, j] < 157.5):
                    q = magnitude[i - 1, j - 1]
                    r = magnitude[i + 1, j + 1]
                
                if (magnitude[i, j] >= q) and (magnitude[i, j] >= r):
                    suppressed_image[i, j] = magnitude[i, j]
                else:
                    suppressed_image[i, j] = 0
        
        return suppressed_image
    
    def double_threshold(image):
        high_threshold = image.max() * 0.2
        low_threshold = high_threshold * 0.5
        
        strong_edges = np.zeros_like(image, dtype=np.uint8)
        weak_edges = np.zeros_like(image, dtype=np.uint8)
        
        # Strong edges: above the high threshold
        strong_edges[image >= high_threshold] = 255
        # Weak edges: between high and low threshold
        weak_edges[(image >= low_threshold) & (image < high_threshold)] = 50
        
        return strong_edges, weak_edges
    
    def edge_tracking_by_hysteresis(strong_edges, weak_edges):
        final_edges = np.copy(strong_edges)
        
        for i in range(1, strong_edges.shape[0]-1):
            for j in range(1, strong_edges.shape[1]-1):
                if strong_edges[i, j] == 255:
                    # Check for weak edges connected to strong edges
                    if (weak_edges[i + 1, j] == 50) or (weak_edges[i - 1, j] == 50) or \
                       (weak_edges[i, j + 1] == 50) or (weak_edges[i, j - 1] == 50) or \
                       (weak_edges[i + 1, j + 1] == 50) or (weak_edges[i - 1, j - 1] == 50) or \
                       (weak_edges[i + 1, j - 1] == 50) or (weak_edges[i - 1, j + 1] == 50):
                        final_edges[i, j] = 255
                    else:
                        final_edges[i, j] = 0
        
        return final_edges
    
    # Example usage
    image = cv2.imread('your_image.jpg', cv2.IMREAD_GRAYSCALE)  # Read the image as grayscale
    edges = canny_edge_detection(image)
    
    # Display the edges
    plt.figure(figsize=(10, 10))
    plt.imshow(edges, cmap='gray')
    plt.title('Edge Detection using Canny')
    plt.axis('off')
    plt.show()
    
</xmp>
<h2>7.2 region growing segmentation</h2>
<xmp>
    import cv2
    import numpy as np
    import matplotlib.pyplot as plt
    
    def region_growing(img, seed, threshold):
        """
        Perform region growing segmentation.
        
        Parameters:
        - img: Input grayscale image
        - seed: Seed point (x, y)
        - threshold: Intensity difference threshold for region growing
        
        Returns:
        - segmented_image: Binary image with segmented region
        """
        # Initialize the output segmented image
        segmented_image = np.zeros_like(img)
        
        # Get image dimensions
        rows, cols = img.shape
        
        # Create a queue to hold the pixels to be processed
        queue = [seed]
        
        # Get the intensity value at the seed point
        seed_value = img[seed]
        
        # Mark the seed point as visited in the segmented image
        segmented_image[seed] = 255
        
        # 8-connected neighborhood (up, down, left, right, and diagonal)
        neighbors = [(-1, 0), (1, 0), (0, -1), (0, 1), (-1, -1), (-1, 1), (1, -1), (1, 1)]
        
        while queue:
            # Pop the first element in the queue
            x, y = queue.pop(0)
            
            # Check the 8 neighbors
            for dx, dy in neighbors:
                nx, ny = x + dx, y + dy
                if 0 <= nx < rows and 0 <= ny < cols and segmented_image[nx, ny] == 0:
                    # If the pixel intensity is within the threshold, add it to the region
                    if abs(int(img[nx, ny]) - int(seed_value)) <= threshold:
                        queue.append((nx, ny))  # Add to the queue
                        segmented_image[nx, ny] = 255  # Mark as part of the region
        
        return segmented_image
    
    # Load the image (grayscale)
    image = cv2.imread('your_image.jpg', cv2.IMREAD_GRAYSCALE)
    
    # Define the seed point and threshold
    seed_point = (50, 50)  # Example seed point (change as needed)
    threshold_value = 30  # Intensity threshold for region growing
    
    # Apply region growing segmentation
    segmented_image = region_growing(image, seed_point, threshold_value)
    
    # Plot the original and segmented images
    plt.figure(figsize=(10, 10))
    
    # Show original image
    plt.subplot(1, 2, 1)
    plt.imshow(image, cmap='gray')
    plt.title('Original Image')
    plt.axis('off')
    
    # Show segmented image
    plt.subplot(1, 2, 2)
    plt.imshow(segmented_image, cmap='gray')
    plt.title('Segmented Image (Region Growing)')
    plt.axis('off')
    
    plt.show()
    
</xmp>
</body>
</html>