<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Logs</title>
</head>
<body>
    <h2>Install these first</h2>
    <xmp>
# Core libraries
pip install opencv-python          # for cv2
pip install numpy                  # for numpy
pip install matplotlib            # for matplotlib
pip install scikit-image          # for skimage
pip install mahotas               # for mahotas
pip install tensorflow            # for tensorflow
pip install scikit-learn          # for sklearn
pip install seaborn              # for seaborn
pip install tensorflow-datasets   # for tensorflow_datasets
    </xmp>
    <h2>1.1 </h2>
    <xmp>
import cv2
import numpy as np
image1_path = r'C:\Users\vicky\OneDrive\Desktop\clg\Labs\7th sem\IPCV\horse.jpg'
image2_path = r'C:\Users\vicky\OneDrive\Desktop\clg\Labs\7th sem\IPCV\image.jpg'
M1 = cv2.imread(image1_path)
M2 = cv2.imread(image2_path)
M2 = cv2.resize(M2, (M1.shape[1], M1.shape[0]))
Out = cv2.absdiff(M1, M2)
cv2.imshow('Absolute Difference', Out)
cv2.waitKey(0)
cv2.destroyAllWindows()
    </xmp>
    <h2>1.2 feature extraction HOG</h2>
    <xmp>
import cv2
import numpy as np
import matplotlib.pyplot as plt
# from skimage.feature import hog
from skimage import exposure
import mahotas
image_path = r'C:\Users\vicky\OneDrive\Desktop\clg\Labs\7th sem\IPCV\image.jpg'
image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
image = cv2.resize(image, (64, 128))
gradient_x = cv2.Sobel(image, cv2.CV_64F, 1, 0,ksize=3)
gradient_y = cv2.Sobel(image, cv2.CV_64F, 0, 1,ksize=3)
print("The gradients are : ", gradient_x, gradient_y)
magnitude = np.sqrt(gradient_x**2 + gradient_y**2)
orientation = np.arctan2(gradient_y, gradient_x) * (180 / np.pi) 
print("Magintude: ", magnitude)
print("Gradient: ",orientation)

from skimage.feature import hog
features, hog_image = hog(image,visualize=True)

hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0,10))

plt.figure()
plt.imshow(image, cmap=plt.cm.gray)
plt.title('Original Image')
plt.axis('off')
plt.show()
plt.figure()
plt.imshow(hog_image_rescaled, cmap=plt.cm.gray)
plt.title('HOG Image')
plt.axis('off')
plt.show()


print(features)
# Print the HOG feature vector length
print("HOG feature vector length:", len(features))
       
    </xmp>
    <h2>2.1 contrast stretching linear filtering</h2>
    <xmp>
import cv2
import numpy as np
import matplotlib.pyplot as plt

# Load the image in grayscale
image_path = r'image.jpg'  # Replace with your image path
image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

# 1) Contrast Stretching
# Find the minimum and maximum pixel values
min_val = np.min(image)
max_val = np.max(image)

# Apply contrast stretching
stretched_image = ((image - min_val) / (max_val - min_val) * 255).astype(np.uint8)

# 2) Linear Filtering (Gaussian filter)
filtered_image = cv2.GaussianBlur(image, (5, 5), 0)

# Show the original image and its histogram
plt.figure(figsize=(6, 6))
plt.imshow(image, cmap='gray')
plt.title('Original Image')
plt.axis('off')
plt.show()

plt.figure(figsize=(6, 6))
plt.hist(image.ravel(), bins=256, range=(0, 255))
plt.title('Histogram of Original Image')
plt.show()

# Show the contrast stretched image and its histogram
plt.figure(figsize=(6, 6))
plt.imshow(stretched_image, cmap='gray')
plt.title('Contrast Stretched Image')
plt.axis('off')
plt.show()

plt.figure(figsize=(6, 6))
plt.hist(stretched_image.ravel(), bins=256, range=(0, 255))
plt.title('Histogram of Contrast Stretched Image')
plt.show()

# Show the filtered image and its histogram
plt.figure(figsize=(6, 6))
plt.imshow(filtered_image, cmap='gray')
plt.title('Filtered Image (Gaussian Blur)')
plt.axis('off')
plt.show()

plt.figure(figsize=(6, 6))
plt.hist(filtered_image.ravel(), bins=256, range=(0, 255))
plt.title('Histogram of Filtered Image')
plt.show()
    </xmp>
<h2>2.2 horse and human clasify</h2>
<xmp>
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
from tensorflow.keras.optimizers import Adam
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, accuracy_score

# Load the dataset
dataset_dir = r"C:\Users\vicky\Downloads\Mid-model Exam\Mid-model Exam\horse-or-human\horse-or-human"  # Replace with actual path
train_data = tf.keras.utils.image_dataset_from_directory(
    dataset_dir,
    image_size=(128, 128),
    batch_size=32,
    label_mode='binary',
    validation_split=0.3,
    subset='training',
    seed=42
)

val_data = tf.keras.utils.image_dataset_from_directory(
    dataset_dir,
    image_size=(128, 128),
    batch_size=32,
    label_mode='binary',
    validation_split=0.3,
    subset='validation',
    seed=42
)
for images, labels in train_data.take(1):  # Take one batch from the test set
    plt.figure(figsize=(10, 10))
    for i in range(9):  
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title(labels[i])
        plt.axis("off")
    plt.show()
# Normalize data
normalization_layer = layers.Rescaling(1.0 / 255)
train_data = train_data.map(lambda x, y: (normalization_layer(x), y))
val_data = val_data.map(lambda x, y: (normalization_layer(x), y))

# Modify ResNet50 for binary classification
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(128, 128, 3))
base_model.trainable = False  # Freeze the base model

# Add custom layers on top
model = models.Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(1, activation='sigmoid')  # Output layer for binary classification
])

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(train_data, validation_data=val_data, epochs=3)

# Evaluate the model on the training data to get training accuracy
train_loss, train_acc = model.evaluate(train_data)
print(f"\nTraining Accuracy: {train_acc:.2f}")

# Evaluate the model on the validation data to get testing accuracy
val_loss, val_acc = model.evaluate(val_data)
print(f"\nValidation (Testing) Accuracy: {val_acc:.2f}")

# Predict labels on the validation dataset
y_pred = np.round(model.predict(val_data).flatten())
y_true = np.concatenate([label.numpy() for _, label in val_data], axis=0)

# Calculate and plot confusion matrix
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['Horse', 'Human'], yticklabels=['Horse', 'Human'])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

</xmp>
<h2>3.1 geometrical transformation</h2>
<xmp>
import cv2
import numpy as np
import matplotlib.pyplot as plt

# Load the image
image_path = r'image.jpg'  # Replace with the path to your image
image = cv2.imread(image_path)
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB for displaying with matplotlib

# Display original image
plt.figure(figsize=(10, 10))
plt.subplot(2, 2, 1)
plt.imshow(image)
plt.title("Original Image")
plt.axis('off')

# 1. Scaling
scale_percent = 50  # Scale image to 50% of original size
width = int(image.shape[1] * scale_percent / 100)
height = int(image.shape[0] * scale_percent / 100)
scaled_image = cv2.resize(image, (width, height), interpolation=cv2.INTER_AREA)

plt.subplot(2, 2, 2)
plt.imshow(scaled_image)
plt.title("Scaled Image (50%)")
plt.axis('off')

# 2. Rotation
angle = 45  # Rotate the image by 45 degrees
(h, w) = image.shape[:2]
center = (w // 2, h // 2)
rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)
rotated_image = cv2.warpAffine(image, rotation_matrix, (w, h))

plt.subplot(2, 2, 3)
plt.imshow(rotated_image)
plt.title("Rotated Image (45 degrees)")
plt.axis('off')

# 3. Shearing
shear_factor = 0.2
M_shear = np.array([[1, shear_factor, 0],
                    [shear_factor, 1, 0],
                    [0, 0, 1]], dtype=float)

sheared_image = cv2.warpPerspective(image, M_shear, (int(w * (1 + shear_factor)), int(h * (1 + shear_factor))))

plt.subplot(2, 2, 4)
plt.imshow(sheared_image)
plt.title("Sheared Image")
plt.axis('off')

# Display all images
plt.tight_layout()
plt.show()

</xmp>
<h2>3.2 feature extraction SIFT</h2>
<xmp>
import cv2
import matplotlib.pyplot as plt

# Load the image
image_path = 'image.jpg'  # Replace with the path to your image
image = cv2.imread(image_path)
gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# Step 1: Constructing a Scale Space and Detecting Keypoints
sift = cv2.SIFT_create()
keypoints = sift.detect(gray_image, None)

# Step 2: Keypoint Localization (part of SIFT's internal process)
# Keypoints are already localized during the `detect` method

# Step 3 & 4: Orientation Assignment and Keypoint Descriptor Computation
keypoints, descriptors = sift.compute(gray_image, keypoints)

# Draw the keypoints on the original image
image_with_keypoints = cv2.drawKeypoints(image, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

# Display the image with keypoints
plt.figure(figsize=(10, 10))
plt.imshow(cv2.cvtColor(image_with_keypoints, cv2.COLOR_BGR2RGB))
plt.title("Image with SIFT Keypoints")
plt.axis('off')
plt.show()

# Print out some information about keypoints and descriptors
print(f"Number of keypoints detected: {len(keypoints)}")
print("Descriptor shape:", descriptors.shape)

</xmp>
<h2>4.1 extract image from video</h2>
<xmp>
import cv2
import os

# Video path
video_path = 'path/to/your/video.mp4'  # Replace with the actual video path
output_folder = 'extracted_images'  # Folder to store extracted images

# Create a folder to store images if it doesn't exist
if not os.path.exists(output_folder):
    os.makedirs(output_folder)

# Open the video file
cap = cv2.VideoCapture(video_path)

# Check if the video was opened successfully
if not cap.isOpened():
    print("Error: Could not open video.")
    exit()

# Get the total number of frames in the video
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
print(f'Total frames in the video: {total_frames}')

frame_count = 0
while True:
    ret, frame = cap.read()
    if not ret:
        break  # Exit if we reach the end of the video

    # Save every frame as an image
    frame_filename = os.path.join(output_folder, f"frame_{frame_count:04d}.jpg")
    cv2.imwrite(frame_filename, frame)
    
    print(f"Extracting frame {frame_count + 1}/{total_frames}")
    frame_count += 1

# Release the video capture object
cap.release()

print("Image extraction completed!")
    
</xmp>
<h2>4.2 horse and human cnn resnet</h2>
<xmp>
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
from tensorflow.keras.optimizers import Adam
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, accuracy_score

# Load the dataset
dataset_dir = r"C:\Users\vicky\Downloads\Mid-model Exam\Mid-model Exam\horse-or-human\horse-or-human"  # Replace with actual path
train_data = tf.keras.utils.image_dataset_from_directory(
    dataset_dir,
    image_size=(128, 128),
    batch_size=32,
    label_mode='binary',
    validation_split=0.3,
    subset='training',
    seed=42
)

val_data = tf.keras.utils.image_dataset_from_directory(
    dataset_dir,
    image_size=(128, 128),
    batch_size=32,
    label_mode='binary',
    validation_split=0.3,
    subset='validation',
    seed=42
)

# Normalize data
normalization_layer = layers.Rescaling(1.0 / 255)
train_data = train_data.map(lambda x, y: (normalization_layer(x), y))
val_data = val_data.map(lambda x, y: (normalization_layer(x), y))

# Modify ResNet50 for binary classification
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(128, 128, 3))
base_model.trainable = False  # Freeze the base model

# Add custom layers on top
model = models.Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(1, activation='sigmoid')  # Output layer for binary classification
])

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(train_data, validation_data=val_data, epochs=3)

# Evaluate the model on the training data to get training accuracy
train_loss, train_acc = model.evaluate(train_data)
print(f"\nTraining Accuracy: {train_acc:.2f}")

# Evaluate the model on the validation data to get testing accuracy
val_loss, val_acc = model.evaluate(val_data)
print(f"\nValidation (Testing) Accuracy: {val_acc:.2f}")

# Predict labels on the validation dataset
y_pred = np.round(model.predict(val_data).flatten())
y_true = np.concatenate([label.numpy() for _, label in val_data], axis=0)

# Calculate and plot confusion matrix
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['Horse', 'Human'], yticklabels=['Horse', 'Human'])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

</xmp>
<h2>5 MS-COCO dataset</h2>
<xmp>
import tensorflow as tf
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt
from tensorflow.keras import layers

# Load the MS-COCO dataset
dataset, info = tfds.load('coco/2017', with_info=True, as_supervised=True)

# Split the dataset into training and validation (testing) sets
train_data = dataset['train']
val_data = dataset['validation']

# Check the number of training and testing images
train_size = info.splits['train'].num_examples
val_size = info.splits['validation'].num_examples

print(f'Training Images: {train_size}')
print(f'Validation Images: {val_size}')

def plot_images(dataset, num_images=5):
    plt.figure(figsize=(10, 10))
    for i, (image, label) in enumerate(dataset.take(num_images)):
        plt.subplot(1, num_images, i + 1)
        plt.imshow(image)
        plt.axis('off')
    plt.show()

plot_images(train_data)

def augment_image(image):
    image = tf.image.random_flip_left_right(image)  # Random horizontal flip
    image = tf.image.random_flip_up_down(image)    # Random vertical flip
    image = tf.image.random_contrast(image, lower=0.2, upper=0.5)  # Random contrast
    image = tf.image.random_rotation(image, 0.2)   # Random rotation
    return image

# Apply augmentation on the dataset
train_data = train_data.map(lambda image, label: (augment_image(image), label))
val_data = val_data.map(lambda image, label: (augment_image(image), label))

normalization_layer = layers.Rescaling(1.0 / 255)
train_data = train_data.map(lambda x, y: (normalization_layer(x), y))
val_data = val_data.map(lambda x, y: (normalization_layer(x), y))

# Build a CNN model
model = tf.keras.Sequential([
    layers.InputLayer(input_shape=(128, 128, 3)),  # Adjust size to 128x128 for simplicity
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(1, activation='sigmoid')  # Binary classification
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(train_data.batch(32), validation_data=val_data.batch(32), epochs=10)


# Plot training and validation accuracy
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

</xmp>
<h2>6 BCCD dataset</h2>
<xmp>
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt
import numpy as np
import os

# a) Load the dataset
base_dir = r".."
train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'valid')

# b) Show the number of testing and training images
def count_images(directory):
    return len([f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))])

train_size = count_images(train_dir)
validation_size = count_images(validation_dir)
print(f"Number of training images: {train_size}")
print(f"Number of validation images: {validation_size}")

# c) Plot some images
def plot_samples(directory, num_images=9):
    plt.figure(figsize=(10, 10))
    for i, img_name in enumerate(os.listdir(directory)[:num_images]):
        img_path = os.path.join(directory, img_name)
        img = tf.keras.preprocessing.image.load_img(img_path, target_size=(150, 150))
        plt.subplot(3, 3, i + 1)
        plt.imshow(img)
        plt.axis('off')
    plt.suptitle('Sample Images from Dataset')
    plt.show()

plot_samples(train_dir)

# d) Do image augmentation â€“ contrast, flipping, and rotation
train_datagen_augmented = ImageDataGenerator(
    rescale=1.0/255,
    rotation_range=20,
    horizontal_flip=True,
    vertical_flip=True,
    brightness_range=[0.8, 1.2]
)

validation_datagen = ImageDataGenerator(rescale=1.0/255)

# Load images with augmentation
train_generator_augmented = train_datagen_augmented.flow_from_directory(
    base_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode='categorical',
    subset='training'  # Specify for augmentation
)

validation_generator = validation_datagen.flow_from_directory(
    base_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode='categorical'
)

# e) After augmentation, show the number of testing and training images
print(f"Number of training images (augmented): {train_size}")
print(f"Number of validation images: {validation_size}")

# Normalize the training data using ImageDataGenerator (rescale=1.0/255).

# g) Build a CNN for training without augmentation
cnn_model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(3, activation='softmax')  # Three classes: WBC, RBC, Platelets
])

cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train CNN without augmentation
history_no_aug = cnn_model.fit(
    validation_generator,  # Without augmentation
    epochs=5,
    validation_data=validation_generator
)

# h) Show training/testing accuracy without augmentation
plt.plot(history_no_aug.history['accuracy'], label='Training Accuracy (No Augmentation)')
plt.plot(history_no_aug.history['val_accuracy'], label='Validation Accuracy (No Augmentation)')
plt.legend()
plt.title("Training and Validation Accuracy without Augmentation")
plt.show()

# i) Build a CNN for training images with augmentation
cnn_model_aug = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D(2, 2),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(3, activation='softmax')  # Three classes
])

cnn_model_aug.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train CNN with augmentation
history_aug = cnn_model_aug.fit(
    train_generator_augmented,  # With augmentation
    epochs=5,
    validation_data=validation_generator
)

# j) Show training/testing accuracy with augmentation
plt.plot(history_aug.history['accuracy'], label='Training Accuracy (With Augmentation)')
plt.plot(history_aug.history['val_accuracy'], label='Validation Accuracy (With Augmentation)')
plt.legend()
plt.title("Training and Validation Accuracy with Augmentation")
plt.show()

# k) Compare training/testing accuracy before and after augmentation
plt.plot(history_no_aug.history['accuracy'], label='Training Accuracy (No Augmentation)')
plt.plot(history_no_aug.history['val_accuracy'], label='Validation Accuracy (No Augmentation)')
plt.plot(history_aug.history['accuracy'], label='Training Accuracy (With Augmentation)')
plt.plot(history_aug.history['val_accuracy'], label='Validation Accuracy (With Augmentation)')
plt.legend()
plt.title("Comparison of Model Accuracy with and without Augmentation")
plt.show()
</xmp>
<h2>7.1 edge detection gray scale</h2>
<xmp>
import cv2
import numpy as np
import matplotlib.pyplot as plt

# Load the image and convert to grayscale
image = cv2.imread('imag1.jpg', cv2.IMREAD_GRAYSCALE)

# Step 1: Smoothing using Gaussian filter
smoothed_image = cv2.GaussianBlur(image, (5, 5), 0)

# Step 2: Gradient calculation using Sobel operators
gradient_x = cv2.Sobel(smoothed_image, cv2.CV_64F, 1, 0, ksize=3)
gradient_y = cv2.Sobel(smoothed_image, cv2.CV_64F, 0, 1, ksize=3)

# Calculate the magnitude and angle of the gradients
magnitude = cv2.magnitude(gradient_x, gradient_y)
angle = cv2.phase(gradient_x, gradient_y, angleInDegrees=True)

# Step 3: Non-Maximum Suppression (thinning edges)
nms_image = np.zeros_like(magnitude)
for i in range(1, magnitude.shape[0] - 1):
    for j in range(1, magnitude.shape[1] - 1):
        # Get angle of the gradient
        ang = angle[i, j] % 180
        if (0 <= ang < 22.5) or (157.5 <= ang <= 180):
            neighbors = [magnitude[i, j+1], magnitude[i, j-1]]
        elif (22.5 <= ang < 67.5):
            neighbors = [magnitude[i+1, j-1], magnitude[i-1, j+1]]
        elif (67.5 <= ang < 112.5):
            neighbors = [magnitude[i+1, j], magnitude[i-1, j]]
        else:
            neighbors = [magnitude[i-1, j-1], magnitude[i+1, j+1]]
        
        # Compare with neighbors and suppress non-maxima
        if magnitude[i, j] >= max(neighbors):
            nms_image[i, j] = magnitude[i, j]
        else:
            nms_image[i, j] = 0

# Step 4: Apply thresholding (Edge tracking by hysteresis)
low_threshold = 50
high_threshold = 150
edges = cv2.inRange(nms_image, low_threshold, high_threshold)

# Display the result
plt.figure(figsize=(10, 10))
plt.subplot(1, 2, 1)
plt.title('Original Image')
plt.imshow(image, cmap='gray')

plt.subplot(1, 2, 2)
plt.title('Canny Edge Detection')
plt.imshow(edges, cmap='gray')
plt.show()
</xmp>
<h2>7.2 region growing segmentation</h2>
<xmp>
import cv2
import numpy as np
import matplotlib.pyplot as plt

# Load the image and convert to grayscale
image = cv2.imread('imag1.jpg', cv2.IMREAD_GRAYSCALE)

# Set the seed point (starting pixel) and threshold for growing the region
seed_point = (100, 100)  # Example seed point
threshold = 10           # Threshold for intensity similarity

# Create an output image initialized with zeros (black)
segmented_image = np.zeros_like(image)

# Function to perform region growing segmentation
def region_growing(img, seed, threshold):
    height, width = img.shape
    visited = np.zeros_like(img, dtype=bool)  # To keep track of visited pixels
    region_intensity = img[seed]  # Intensity value at the seed point
    region = [seed]
    
    # A simple 4-connected neighborhood system (can be expanded to 8-connectivity)
    neighbors = [(-1, 0), (1, 0), (0, -1), (0, 1)]
    
    # While there are still pixels to process
    while region:
        x, y = region.pop()
        if visited[x, y]:
            continue
        visited[x, y] = True
        
        # Check intensity difference with the seed point
        if abs(int(img[x, y]) - region_intensity) <= threshold:
            segmented_image[x, y] = img[x, y]
            
            # Add the neighbors to the region
            for dx, dy in neighbors:
                nx, ny = x + dx, y + dy
                if 0 <= nx < height and 0 <= ny < width and not visited[nx, ny]:
                    region.append((nx, ny))

# Run region-growing segmentation
region_growing(image, seed_point, threshold)

# Display the segmented image
plt.figure(figsize=(10, 10))
plt.subplot(1, 2, 1)
plt.title('Original Image')
plt.imshow(image, cmap='gray')

plt.subplot(1, 2, 2)
plt.title('Segmented Image (Region Growing)')
plt.imshow(segmented_image, cmap='gray')
plt.show()
</xmp>
</body>
</html>
